{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798c2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/texnedo/Documents/projects/mathematics-and-python/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/texnedo/Documents/projects/mathematics-and-python/venv/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import random as  rnd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9358cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ace4cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        # remove leading and trailing whitespace\n",
    "        pure_line = line.strip()#.lower()\n",
    "\n",
    "        # if pure_line is not the empty string,\n",
    "        if pure_line:\n",
    "            # append it to the list\n",
    "            lines.append(pure_line)\n",
    "            \n",
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "917faa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 unique characters\n",
      "[UNK]  \t \n",
      "   ! $ & ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z |\n"
     ]
    }
   ],
   "source": [
    "text = \"\\n\".join(lines)\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "vocab.insert(0,\"[UNK]\") # Add a special character for any unknown\n",
    "vocab.insert(1,\"\") # Add the empty character for padding.\n",
    "\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(\" \".join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e4ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, vocab):\n",
    "    chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
    "    ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f62179f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids, vocab):\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7e91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78072f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    single_line_data  = \"\\n\".join(lines)\n",
    "    all_ids = line_to_tensor(single_line_data, vocab)\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True) \n",
    "    dataset_xy = data_generator.map(split_input_target)\n",
    "    dataset = (                                   \n",
    "        dataset_xy                                \n",
    "        .shuffle(seq_length)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "        )            \n",
    "                                     \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97bda09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 124097\n",
      "Number of validation lines: 1000\n"
     ]
    }
   ],
   "source": [
    "train_lines = lines[:-1000] # Leave the rest for training\n",
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "\n",
    "print(f\"Number of training lines: {len(train_lines)}\")\n",
    "print(f\"Number of validation lines: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ad9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "dataset = create_batch_dataset(train_lines, vocab, seq_length=250, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59f96c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128, num_layers=2):\n",
    "        super(GRULM, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.gru_layers = [\n",
    "            tf.keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        new_states = []\n",
    "        for i, gru in enumerate(self.gru_layers):\n",
    "            state = states[i] if states is not None else None\n",
    "            x, state = gru(x, initial_state=state, training=training)\n",
    "            new_states.append(state)\n",
    "        logits = self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return logits, new_states\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecb501a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    sample_input = tf.random.uniform((BATCH_SIZE, 100))  \n",
    "    model(sample_input)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e8f56896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRULM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=256,\n",
    "    rnn_units=512,\n",
    "    num_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6010f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"grulm_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"grulm_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                    │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">42,066</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m20,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_17 (\u001b[38;5;33mGRU\u001b[0m)                    │ ((\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m32\u001b[0m,  │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m))                  │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m82\u001b[0m)          │        \u001b[38;5;34m42,066\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,245,778</span> (4.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,245,778\u001b[0m (4.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,245,778</span> (4.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,245,778\u001b[0m (4.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 895ms/step - loss: 2.5139\n",
      "Epoch 2/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 901ms/step - loss: 1.6665\n",
      "Epoch 3/10\n",
      "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 2s/step - loss: 1.5025\n",
      "Epoch 4/10\n",
      "\u001b[1m286/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m49s\u001b[0m 2s/step - loss: 1.4257"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "model = compile_model(model)\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "tf.random.set_seed(272)\n",
    "gen = GenerativeModel(model, vocab, temperature=0.5)\n",
    "\n",
    "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
    "print(gen.generate_n_chars(32, \"Dear\"), '\\n\\n' + '_'*80)\n",
    "print(gen.generate_n_chars(32, \"KING\"), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
